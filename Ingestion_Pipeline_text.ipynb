{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import os\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings, SimpleDirectoryReader, StorageContext, SummaryIndex, VectorStoreIndex\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.2)\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\") # dimensions: 1536"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "load wiki information and datalineage information separately. (.md files)\n",
    "Thus far, we merely load the databases using a markdown parser (already included in SimpleDirectoryReader)\n",
    "In the future we may consider loading images and tables differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents_wiki = SimpleDirectoryReader(\"database_wiki\").load_data()\n",
    "documents_datalineage = SimpleDirectoryReader(\"database_datalineage\").load_data()\n",
    "\n",
    "# initialize settings (set chunk size)\n",
    "Settings.chunk_size = 1024\n",
    "nodes_wiki = Settings.node_parser.get_nodes_from_documents(documents_wiki)\n",
    "nodes_datalineage = Settings.node_parser.get_nodes_from_documents(documents_datalineage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timstrohmeyer/Desktop/Internal Rag/Agentic_RAG/agentic_rag/lib/python3.10/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 3}},\n",
       " 'total_vector_count': 3}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=\"0b599dcc-57d9-4d1a-8c46-d245c8d1a7ec\")\n",
    "\n",
    "# Create Index\n",
    "wiki_index_name = \"internalrag-mini-wiki\"\n",
    "datalineage_index_name = \"internalrag-mini-datalineage\"\n",
    "\n",
    "'''\n",
    "# Delete Index\n",
    "if index_name in [index.name for index in pc.list_indexes()]:\n",
    "    pc.delete_index(index_name)\n",
    "    \n",
    "# Create Index\n",
    "pc.create_index(\n",
    "    name=datalineage_index_name,\n",
    "    dimension=1536, # Replace with your model dimensions\n",
    "    metric=\"cosine\", # Replace with your model metric\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\" # Note: free plan does not support indexes in the eu-west-1 region of aws\n",
    "    ) \n",
    ")\n",
    "'''\n",
    "\n",
    "index = pc.Index(datalineage_index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# initialize storage contexts (by default it's in-memory)\n",
    "# Create Index\n",
    "wiki_index = pc.Index(wiki_index_name)\n",
    "datalineage_index = pc.Index(datalineage_index_name)\n",
    "\n",
    "# Wiki storage\n",
    "vector_store_wiki = PineconeVectorStore(pinecone_index=wiki_index)\n",
    "storage_context_wiki = StorageContext.from_defaults(vector_store=vector_store_wiki)\n",
    "storage_context_wiki.docstore.add_documents(nodes_wiki)\n",
    "\n",
    "# Datalineage storage\n",
    "vector_store_datalineage = PineconeVectorStore(pinecone_index=datalineage_index)\n",
    "storage_context_datalineage = StorageContext.from_defaults(vector_store=vector_store_datalineage)\n",
    "storage_context_datalineage.docstore.add_documents(nodes_datalineage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc chunk: 0\n",
      "Doc id:  DataCatalog.md\n",
      "\n",
      "\n",
      "Curated Zone\n",
      "\n",
      "Schema Name: emea_cz\n",
      "Doc chunk: 1\n",
      "Doc id:  DataCatalog.md\n",
      "\n",
      "\n",
      "Dimensions\n",
      "\n",
      "Last update: 2024.05.09\n",
      "| Table Name | Associated Dimensions | Description |\n",
      "|:--|:--|:--|\n",
      "|dbid_billing_document|  |Contains descriptive information about billing documents, payment status payment terms, and detailing transaction dates. |\n",
      "|dacc_account|  |Holds account information including account status, type and structure.|\n",
      "|dcus_dm_80_customer_v|  |Contains customer information like demographies, geograhpies, industires and company wide classifications. |\n",
      "|dmat_material_v|  |Contains material master information containing details on materials, such as descriptions, categories and weight information.|\n",
      "|dpor_purchase_order|  |Contains Purchase Order information like receiving, preparation and shipping dates, vendor, shipper and receiver details as well as references to the content of the Purchase.|\n",
      "\n",
      "Last update: 2024.05.10\n",
      "Doc chunk: 2\n",
      "Doc id:  DataCatalog.md\n",
      "\n",
      "\n",
      "Fact\n",
      "| Table Name | Associated Dimensions | Description |\n",
      "|:--|:--|:--|\n",
      "|fcur_currency_rate_hyperion|dcur_currency|Stores currency exchange rates, and links the reference date, scenario, and rate type.|\n",
      "|ffco_cooo_cooc|  |Contains details of cost center operations, including cost allocations and related financials and links the relevant customers and dates.|\n",
      "|finv_dm_95_snapshot_after_kitting_base|  |Snapshot table capturing state of inventory post-kitting process, holding information about inventory balances and links materials and dates.|\n",
      "|fpor_dm_80_billed_purchase_order_all_v|dpor_purchase_order|Contains all billed purchase orders their amount and value and links to the cusomter, shipping center and date information.|\n",
      "Doc chunk: 3\n",
      "Doc id:  Micro_Batch_Loading_Framework.md\n",
      "\n",
      "\n",
      "Micro Batch Loading Framework for the Curated Zone\n",
      "\n",
      "\n",
      "\n",
      "New transactions are constantly being added to the Standardized Zone and are created with a HKey and a timestamp.\n",
      "\n",
      "The transaction timestamp always ascends. So, if the highest currently-extracted timestamp is known, it’s easy to select only the newer transactions. This is the fundamental technique underpinning the SQL query which extracts new records from the SZ.\n",
      "\n",
      "At any point in time the replicated data will be slightly out-of-date compared to the SZ schema, due to the new transactions which have just been added. Every small batch adds a small increment of transactions to the replicated table.\n",
      "\n",
      "Over time, doing this repeatedly will accumulate a large amount of data. However the individual data loads only need to deal with a small number of rows. This makes the incremental data extraction very fast, and enables the microbatching to keep very close to real time.\n",
      "\n",
      "Doc chunk: 4\n",
      "Doc id:  Micro_Batch_Loading_Framework.md\n",
      "\n",
      "\n",
      "Components\n",
      "!CR_MicroBatchLoadingFramework-Microbatch Loading Framework.png\n",
      "\n",
      "Doc chunk: 5\n",
      "Doc id:  Micro_Batch_Loading_Framework.md\n",
      "\n",
      "\n",
      "Dependency Configuration\n",
      "- Map of Source Tables to Job Names in table stored in synpase\n",
      "- Matillion Jobs are only allowed to depend on Vaultspeed Jobs, all other dependencies should be as Views in Synapse to reduce complexity - exception refernece tables - Job dependency in Matillion also work but should not be considered in the feature.\n",
      "- All jobs must be driven by a „driving“ source table. This source table defines the delta records to be loaded. The \"driving\" source tables are mapped in the last_source_table column of the TABLE_LINEAGE.\n",
      "\n",
      "Doc chunk: 6\n",
      "Doc id:  Micro_Batch_Loading_Framework.md\n",
      "\n",
      "\n",
      "Standardized Zone Pipeline Notifier\n",
      "- Each job loading a Vaultspeed source must notify the update time windows of the last data load\n",
      "- Notification must be an asynchronous job to decouple architecture building blocks (here Vaultspeed and Matillion)\n",
      "- Notification steps in the pipeline should be created automatically (e.g. by the CI/CD pipeline) and not be coded by the developers.\n",
      "- Design Option 1) Notification steps issues a message to Blob storage queue with the source table name and the earliest timestamp of the micro batch\n",
      "- Design Option 2) Notification steps issues a message to Blob storage queue with the target Matillion Job Name based on the Dependency Configuration and the earliest timestamp of the micro batch\n",
      "\n",
      "Doc chunk: 7\n",
      "Doc id:  Micro_Batch_Loading_Framework.md\n",
      "\n",
      "\n",
      "Matillion Job Orchestration\n",
      "- Matillion listens on the Blob storage queue and triggers an Orchestration Job\n",
      "- Design Option 1)\n",
      "\t- Table for Dependency Configuration is loaded\n",
      "\t- Get source table from pipeline notification event and look up target Matillion job via grid variable\n",
      "\t- Call target Matillion job and pass earliest micro batch timestamp to target job as job variable\n",
      "\n",
      "Doc chunk: 8\n",
      "Doc id:  Micro_Batch_Loading_Framework.md\n",
      "\n",
      "\n",
      "Matillion target job requirements\n",
      "- each target job has a variable for earliest micro batch timestamp\n",
      "- The default timestamp „1900-01-01 00:00:00“ is treated as full load\n",
      "- each target job has a driving table (or a join/union of tables) where a filter is applied to the data vault satellites: load timestamp >= earliest micro batch timestamp\n",
      "\n",
      "Doc chunk: 9\n",
      "Doc id:  Micro_Batch_Loading_Framework.md\n",
      "\n",
      "\n",
      "Prerequisites\n",
      "- Azure Blob storage queue configured\n",
      "- Matillion project is setup to listen on Blobl Storage queue\n",
      "- Table lineage is with for the latest statest of the DEV/UAT/PRD environments (VaultSpeed and Matillion metadata are joined and up-to-date, Table lineage automation requires matillion git repository and AzureDevOps Pipeline)\n",
      "- python is installed on the Matillion machine\n",
      "\n",
      "\n",
      "!OBIS DnA DataFlow (1).png-099b9cc7-aa26-4871-bacf-1a90041cb68f.png)\n",
      "Doc chunk: 10\n",
      "Doc id:  NamingConventions.md\n",
      "[[_TOC_]] \n",
      "\n",
      "Doc chunk: 11\n",
      "Doc id:  NamingConventions.md\n",
      "\n",
      "\n",
      "Database & Schemas\n",
      "|Entity|Topic|Abbreviation|Reference|\n",
      "|---|---|---|---|\n",
      "|Database|User|PROJECT_ENVIRONMENT_LAYER_WORD|5-10 digits ≠ *#.%&:\\\\+?/|\n",
      "|Database|Warehouse|PROJECT_ENVIRONMENT_LAYER_WH||\n",
      "|Database|Database|PROJECT_ENVIRONMENT_DB||\n",
      "|Database|Schema|PROJECT_LAYER_WORD||\n",
      "|Database|Trigger Prefix|TR|5-10 digits ≠ *#.%&:\\\\+?/|\n",
      "|Database|Schedule Prefix|RUN|5-10 digits ≠ *#.%&:\\\\+?/|\n",
      "|Environment|Schedule Prefix|DEV|Devlopement environment|\n",
      "|Environment|Schedule Prefix|UAT|User accepting test environment|\n",
      "|Environment|Schedule Prefix|RUN|Production environment|\n",
      "|Dataflow Layer|Raw Zone|RZ|Staged source tables|\n",
      "|Dataflow Layer|Standardized Zone|SZ|DataVault tables|\n",
      "|Dataflow Layer|Curated Zone|CZ|DataMart tables|\n",
      "\n",
      "Doc chunk: 12\n",
      "Doc id:  NamingConventions.md\n",
      "\n",
      "\n",
      "Tables\n",
      "- Table name in Data Vault: - Example: HUB_ORDER\n",
      "- Table name in Data Mart: - Example DORD_ORDER\n",
      "- All tables contain one attribute with suffix _HKEY (Table name_HKEY; e.g. DORD_ORDER_HKEY) Uwhich contains the HK of the respective source (Satellite)\n",
      "- All tables contain a attribute load_date which contains the date of the gration of the row. For the datamart the load_date from the Vault will be taken\n",
      "\n",
      "|Entity|Topic|Abbreviation|Reference|\n",
      "|---|---|---|---|\n",
      "|Table prefix|Satellite|SAT||\n",
      "|Table prefix|Computed satellites|CSAT||\n",
      "|Table prefix|Hub|HUB||\n",
      "|Table prefix|Link|LNK||\n",
      "|Table prefix|Link satellite|LKS||\n",
      "|Table prefix|Dimension|DIM||\n",
      "|Table prefix|Bridge|BRG||\n",
      "|Table prefix|Facttable|F||\n",
      "|Table prefix|PIT|PIT||\n",
      "|Table prefix|External table|EXT||\n",
      "|Table prefix|Temporary tables|TMP||\n",
      "|Table prefix|Materialized view|MV||\n",
      "|Table prefix|Views|V||\n",
      "\n",
      "Doc chunk: 13\n",
      "Doc id:  NamingConventions.md\n",
      "\n",
      "\n",
      "VaultSpeed\n",
      "|Entity|Topic|Abbreviation|Reference|\n",
      "|---|---|---|---|\n",
      "|VaultSpeed Schema|FL|[PROJECT]_[LAYER]_FL|1-128 digits ≠ *#.%&:\\\\+?/|\n",
      "|VaultSpeed Schema|CDC|[PROJECT]_[LAYER]_CDC_[SOURCE]|1-128 digits ≠ *#.%&:\\\\+?/|\n",
      "|VaultSpeed Schema|FMC|[PROJECT]_[LAYER]_FMC|1-128 digits ≠ *#.%&:\\\\+?/|\n",
      "|VaultSpeed Schema|MTD|[PROJECT]_[LAYER]_MTD_[SOURCE]|1-128 digits ≠ *#.%&:\\\\+?/|\n",
      "|VaultSpeed Schema|BV|[PROJECT]_[LAYER]_BV_[SOURCE]|1-128 digits ≠ *#.%&:\\\\+?/|\n",
      "|VaultSpeed Schema|DFV|[PROJECT]_[LAYER]_DFV_[SOURCE]|1-128 digits ≠ *#.%&:\\\\+?/|\n",
      "|VaultSpeed Schema|FL_MIGRATION|[PROJECT]_[LAYER]_FL_MIGRATION_[SOURCE]|1-128 digits ≠ *#.%&:\\\\+?/|\n",
      "|VaultSpeed Schema|EXT|[PROJECT]_[LAYER]_EXT_[SOURCE]|1-128 digits ≠ *#.%&:\\\\+?/|\n",
      "|VaultSpeed Schema|INI|[PROJECT]_[LAYER]_INI_[SOURCE]|1-128 digits ≠ *#.%&:\\\\+?/|\n",
      "|VaultSpeed Schema|PROC|[PROJECT]_[LAYER]_PROC|1-128 digits ≠ *#.%&:\\\\+?/|\n",
      "|VaultSpeed Schema|STG|[PROJECT]_[LAYER]_STG_[SOURCE]|1-128 digits ≠ *#.%&:\\\\+?/|\n",
      "\n",
      "Doc chunk: 14\n",
      "Doc id:  NamingConventions.md\n",
      "\n",
      "\n",
      "Matillion\n",
      "\n",
      "Doc chunk: 15\n",
      "Doc id:  NamingConventions.md\n",
      "\n",
      "\n",
      "Columns\n",
      "- Column names in SNAKE_CASE - Example: ORDER_DATE\n",
      "\n",
      "|Entity|Topic|Abbreviation|Reference|\n",
      "|---|---|---|---|\n",
      "|Column Suffix|HASH KEY attributes (hash key, binary, varbinary)|HKEY||\n",
      "|Column Suffix|TIMESTAMP attributes (timestamp)|TIMESTAMP||\n",
      "|Column Suffix|TIME attributes (time)|TIME||\n",
      "|Column Suffix|DATE attributes (date)|DATE||\n",
      "|Column Suffix|Describing attributes (description)|DESC||\n",
      "|Column Suffix|German language attributes (in german)|GER||\n",
      "|Column Suffix|English language attributes (in english)|ENG||\n",
      "|Column Suffix|Code attributes (code, sign, id)|ID||\n",
      "|Column Suffix|Flag attribute (Flag number)|FG||\n",
      "|Column Suffix|Percentage attribute (percentage)|PCT||\n",
      "|Column Suffix|Number attribute (number)|NO||\n",
      "|Column Suffix|Temporary|TMP||\n",
      "|Column Suffix|Document Currency (number)|DC||\n",
      "|Column Suffix|Local Currency (number)|LC||\n",
      "|Column Suffix|Euro Currency (number)|EC||\n",
      "\n",
      "Doc chunk: 16\n",
      "Doc id:  NamingConventions.md\n",
      "\n",
      "\n",
      "Jobs\n",
      "- Job names in SNAKE_CASE - Example: TRN_F_SALES\n",
      "\n",
      "|Entity|Topic|Abbreviation|Reference|\n",
      "|---|---|---|---|\n",
      "|Job Prefix|Transformation|TRN|Transformation in Matillion|\n",
      "|Job Prefix|Shared Jobs|SHR|Shared Jobs in Matillion|\n",
      "|Pipeline|Orchestration|ORC|Orchestration in Matillion|\n",
      "\n",
      "Doc chunk: 17\n",
      "Doc id:  NamingConventions.md\n",
      "\n",
      "\n",
      "Variables\n",
      "- Variables in SNAKE_CASE - Example: E_SCHEMA_CZ\n",
      "\n",
      "|Entity|Topic|Abbreviation|Reference|\n",
      "|---|---|---|---|\n",
      "|Job variable|Prefix|J||\n",
      "|Job variable|Suffix|C|Copied|\n",
      "|Job variable|Suffix|S|Shared|\n",
      "|grid variable|Prefix|G||\n",
      "|grid variable|Suffix|C|Copied|\n",
      "|grid variable|Suffix|S|Shared|\n",
      "|Environment variable|Prefix|E||\n",
      "|Environment variable|suffix|C|Copied|\n",
      "|Environment variable|suffix|S|Shared|\n",
      "Doc chunk: 18\n",
      "Doc id:  NamingConventions.md\n",
      "\n",
      "\n",
      "Environment Variables\n",
      "All of them are type _text_ and have a _shared_ behaviour\n",
      "\n",
      "|Name|OLY_EMEA_OBIS_DEV|\n",
      "|---|---|\n",
      "|E_EMEA_ADLS_STORAGE|eudevstgobis|\n",
      "|E_EMEA_BLOB_CON_STAGING|eudevconsf|\n",
      "|E_EMEA_SCHEMA_CREF|emea_obis_bv|\n",
      "|E_EMEA_SCHEMA_CSATS|emea_obis_bv|\n",
      "|E_EMEA_SCHEMA_CZ|emea_cz|\n",
      "|E_EMEA_SCHEMA_CZ_TMP|emea_cz_tmp|\n",
      "|E_EMEA_SCHEMA_REF|emea_obis_ref_fl|\n",
      "|E_EMEA_SCHEMA_SZ|emea_obis_dv_fl|\n",
      "|E_EMEA_SCHEMA_SZ_BV|emea_obis_bv|\n",
      "|E_EMEA_SCHEMA_TECHSUP|emea_cz_techsup|\n",
      "\n",
      "Doc chunk: 19\n",
      "Doc id:  NamingConventions.md\n",
      "\n",
      "\n",
      "GiT Branches\n",
      "|Entity|Topic|Abbreviation|Reference|\n",
      "|---|---|---|---|\n",
      "|Git|Branch to solve user story|FXXXX/USXXXXX||\n",
      "|Git|Branch to solve bugs|FXXXX/BFXXXX||\n",
      "\n",
      "Doc chunk: 20\n",
      "Doc id:  NamingConventions.md\n",
      "\n",
      "\n",
      "Components\n",
      "- Component names in CamelCase - Example: JnrOrderPartner\n",
      "\n",
      "|Entity|Topic|Abbreviation|Reference|\n",
      "|---|---|---|---|\n",
      "|SQL Documentation|FUNCTIONAL_SCENARIO|WordWord|Describes the part of the statement added to the job|\n",
      "|SQL Documentation|Aggregate Component|Agg[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Assert View Component|Ast[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Calculator Component|Cal[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Construct Struct Component|Cns[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Construct Variant|Cnv[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Convert Type Component|Cnt[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Delete Rows Component|Del[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Detect Changes Component|Dtc[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Distinct Component|Dis[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Extract Nested Data|Exn[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Filter Component|Flt[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|First/Last Component|Fnl[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Flatten Variant Component|Ftv[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Lead/Lag Component|Lnl[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Map Values Component|Mpv[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Pivot Component|Pvt[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Rank Component|Rnk[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Rename Component|Ren[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Replicate Component|Rep[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Split Field Component|Spl[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|SQL Component|Sql[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Transpose Columns|Trc[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Transpose Rows|Trr[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Window Calculation Component|Wic[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Unpivot Component|Unp[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Join Component|Jon[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Unite Component|UNI_[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Intersect Component|Int[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Except Component|Exc[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Fixed Flow Component|Fxf[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Generate Sequence Component|Seq[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Multi Table Input Component|Mtb[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Stream Input|Sti[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Table Input Component|Tbi[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Wildcard Table Input|Wit[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Copy Table To External Schema|Cpt[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Create View Component|Crv[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|External Table Output|Ext[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Rewrite Table Component|Wrt[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Rewrite External Table|Wre[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Table Output Component|Tbo[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Table Update Component|Tbu[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Assert External Table Component|Aet[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Assert Scalar Variables Component|Avt[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Assert Table Component|Ast[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|API Extract|Api[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|API Query|Apq[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Azure SQL Query|Azq[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Azure Blob Storage Load|Azb[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Azure Blob Storage Unload|Azu[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Create External Table|Cet[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Create Table Component|Crt[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Delete Tables Component|Del[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|SQL Script Component|Sqc[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Truncate Tables Component|Trt[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|File Iterator Component|Fit[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Fixed Iterator Component|Fxi[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Grid Iterator Component|Gri[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Loop Iterator|Lop[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Table Iterator Component|Tbi[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|And Component|And[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|End Failure Component|Enf[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|End Success Component|Ens[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|If component|If[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|OR component|Or[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Retry Component|Ret[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Run Orchestration Component|Ror[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Run Transformation Component|Rtr[FUNCTIONAL_SCENARIO]||\n",
      "|SQL Documentation|Python Script Component|Pys[FUNCTIONAL_SCENARIO]||\n",
      "\n",
      "Doc chunk: 21\n",
      "Doc id:  NamingConventions.md\n",
      "\n",
      "\n",
      "Qlik Replicate\n",
      "Qlik Replicate Tasks and Endpoints are following the naming convention predefined in DnA project.  \n",
      "\n",
      "Doc chunk: 22\n",
      "Doc id:  NamingConventions.md\n",
      "\n",
      "\n",
      "Qlik Replicate Endpoint Naming convention\n",
      "The following factors are considered while arriving at this format.  \n",
      "1. There are two types of endpoints in Qlik Replicate.  Source & Target.\n",
      "2. Three environments in DnA - DEV, UAT & PRD\n",
      "3. Category of the source or the target system - SYN, ADLS, SAPEXT, ORA, IDB2, LDB2, SQL for Azure Synapse, Data Lake storage, SAP Extractor, Oracle, IBM DB2 on iSeries, IBM DB2 on Linux and MS SQL respectively\n",
      "4. Name of the Application - OURANUS, JUPITER, RZ (Raw Zone), NEOGAIACRM, NEOGAIAECC etc. \n",
      "\n",
      "Some examples are given below.\n",
      "- TARGET-DEV-SYN-RZ\n",
      "- SOURCE-UAT-SAPEXT-OEKGCRM\n",
      "- TARGET-PRD-ADLS-OBIS\n",
      "- SOURCE-DEV-IDB2-SALES\n",
      "- SOURCE-UAT-SQL-HFM\n",
      "- SOURCE-PRD-ORA-EBS\n",
      "\n",
      "Doc chunk: 23\n",
      "Doc id:  NamingConventions.md\n",
      "\n",
      "\n",
      "Qlik Replicate Task Naming convention\n",
      "The following factors are considered while arriving at this format.  \n",
      "1. There could be multiple tasks for the same pair of source and target endpoints. Some tables loaded using task1 & other tables using task2.  \n",
      "2. Name of the application in-line with the Source endpoint.  \n",
      "\n",
      "Some examples are given below.\n",
      "- TASK-DEV-OEKGCRM-01\n",
      "- TASK-DEV-OSTECCEXT-01\n",
      "- TASK-DEV-JUPSAPDB-01\n",
      "- TASK-DEV-MBCSAPEXT-02\n",
      "- TASK-DEV-HFM-01\n",
      "Doc chunk: 24\n",
      "Doc id:  NamingConventions.md\n",
      "\n",
      "\n",
      "Theobald\n",
      "\n",
      "Doc chunk: 25\n",
      "Doc id:  NamingConventions.md\n",
      "\n",
      "\n",
      "Schemas\n",
      "| Source        | Description                                        | Target Schema |\n",
      "|---------------|----------------------------------------------------|---------------|\n",
      "| SAP CRM       | Customer Relationship Management Module            | RZ_SAPCRM \n",
      "| SAP CRM Ref   | Reference tables from Customer Relationship Module | RZ_SAPCRM_REF\n",
      "| SAP OW Par    | Logistics Partner information from SAP OW System   | RZ_SAPOW_PAR\n",
      "| SAP OE Par    | Logistics Partner Information from SAP OE system   | RZ_SAPOE_PAR\n",
      "| SAP OE MM Pur | Materials Management Purchasing from SAP OE System | RZ_SAPOE_MMPUR\n",
      "| SAP OW MM Pur | Materials Management Purchasing from SAP OW System |RZ_SAPOW_MMPUR\n",
      "| SAP OE SD     | Sales and Distribution from SAP OE System          | RZ_SAPOE_SD\n",
      "| SAP OW SD     | Sales and Distribution from SAP OW System          |RZ_SAPOW_SD\n",
      "| SAP OE MM | Materials Management from SAP OE System | RZ_SAPOE_MM\n",
      "|SAP OW MM| Materials Management from SAP OW System | RZ_SAPOW_MM\n",
      "| SAP OE FICO | Financial Accounting and Controlling from SAP OE system | RZ_SAPOE_FICO\n",
      "| SAP OW FICO | Financial Accounting and Controlling from SAP OW system |RZ_SAPOW_FICO\n",
      "| SAP OE REF | Reference data from SAP OE System module-overarching | RZ_SAPOE_REF\n",
      "|SAP OW REF | Reference data from SAP OW System module-overraching |RZ_SAPOW_FICO|\n",
      "\n",
      "Doc chunk: 26\n",
      "Doc id:  NamingConventions.md\n",
      "\n",
      "\n",
      "Extraction naming\n",
      "\n",
      "System abbreviation + Table or extractor name + Interval name (If partitioned on a date field)\n",
      "\n",
      "In general, most of the extractions are partitioned on a date field they have and it is either monthly, quarterly or yearly.\n",
      "\n",
      "Doc chunk: 27\n",
      "Doc id:  NamingConventions.md\n",
      "\n",
      "\n",
      "Exceptions\n",
      "1. For the sources that had to be partitioned but didn't have any date field,  below convention has been used.\n",
      "\n",
      "System abbreviation + Table or extractor name + Sequenced integers\n",
      "\n",
      "2. Data from CDPOS table has been filtered on TABNAME (Table name columns). Naming convention follows\n",
      "\n",
      "System abbreviation + CDPOS + TABNAME filter\n",
      "\n",
      "Doc chunk: 28\n",
      "Doc id:  NamingConventions.md\n",
      "\n",
      "\n",
      "Examples\n",
      "- OE_MBEW_2022 -> Points to MBEW table 2022 in the OEP system.\n",
      "- OC_STXH_2017_H2 -> Points to STXH table 2017 second half in the OCP system.\n",
      "- OW_JCDS_2020_Q4 -> Points to JCDS table 2020 last quarter in the OWP system.  \n",
      "- OC_ZCRM_CONDITIONS_2020_12 -> Points to ZCRM_CONDITIONS extractor for last month of 2012.\n",
      "- OE_LTAP_1 -> Partitioned on document number field. Points to first bucket.\n",
      "- OW_CDPOS_LIKP -> Extracts data with 'LIKP' in TABNAME column.\n",
      "\n",
      "Doc chunk: 29\n",
      "Doc id:  NamingConventions.md\n",
      "\n",
      "\n",
      "Destination folders and files naming\n",
      "\n",
      "Every extraction is written as a parquet file in the BLOB storage. Naming of these folders and files is configured in Theobald. Rules are;\n",
      "1. Every extraction must have a folder name on its SAP Object name.\n",
      "3. Non-partitioned extractions must use SAP object name as the file name.\n",
      "2. Partitions should use name of the extractions as the file name. \n",
      "\n",
      "!image.png\n",
      "\n",
      "Doc chunk: 30\n",
      "Doc id:  NamingConventions.md\n",
      "\n",
      "\n",
      "Reference Data\n",
      "\n",
      "Reference Data is used for mappings in data integration process.\n",
      "\n",
      "Use schema to manage reference data that is not managed by source systems.\n",
      "RZ_REFDATA\n",
      "\n",
      "Table name starts with ‘REF_’\n",
      "\n",
      "- Contains at least this attributes\n",
      "\n",
      "\n",
      "```\n",
      "Code varchar notnull\n",
      "\n",
      "Value varchar notnull\n",
      "\n",
      "```\n",
      "\n",
      "- Always define a default value (code according to standardized zone ghost records)\n",
      "\n",
      "- Can contain additional attributes for\n",
      "\n",
      "  - Defining harmonized target code\n",
      "\n",
      "  - Grouping values or parent child identifiers\n",
      "\n",
      "  - Descriptions\n",
      "\n",
      "  - Sort orders\n",
      "\n",
      "  - Alternative texts\n",
      "\n",
      "  - Translations to other languages\n",
      "\n",
      "- Must contain technical attributes for\n",
      "\n",
      "  - Created_DATE datetime\n",
      "\n",
      "  - Modified_DATE datetime\n",
      "\n",
      "  - Modified_User varchar\n",
      "\n",
      "  - Valid_FROM (if history must be maintained) datetime\n",
      "\n",
      "  - Valid_TO  (if history must be maintained) datetime\n",
      "\n",
      "Code is the business key, not a hashkey. Code is used in the dimension or fact tables. Value is the text that is shown in the reports.\n",
      "\n",
      "Doc chunk: 31\n",
      "Doc id:  NamingConventions.md\n",
      "\n",
      "\n",
      "Parameter Data\n",
      "\n",
      "Parameter Data is managing functionality on any front end application or business process. Parameter data maintained in the schema RZ_REFDATA and tables will start with PARAM_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at document chunks created\n",
    "\n",
    "for i in range(len(documents_wiki)):\n",
    "    print(f\"Doc chunk: {i}\")\n",
    "    filename = documents_wiki[i].metadata[\"file_name\"]\n",
    "    print(f\"Doc id:  {filename}\")\n",
    "    print(documents_wiki[i].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vector Indices\n",
    "\n",
    "create 2 sepearte Vector indices for Wiki and Datalineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upserted vectors: 100%|██████████| 33/33 [00:02<00:00, 16.41it/s]\n",
      "Upserted vectors: 100%|██████████| 3/3 [00:01<00:00,  2.92it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index_wiki = VectorStoreIndex(nodes_wiki, storage_context=storage_context_wiki)\n",
    "vector_index_datalineage = VectorStoreIndex(nodes_datalineage, storage_context=storage_context_datalineage)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=\"0b599dcc-57d9-4d1a-8c46-d245c8d1a7ec\")\n",
    "\n",
    "# Initialize your index \n",
    "wiki_index = pc.Index(\"internalrag-mini-wiki\")\n",
    "datalineage_index = pc.Index(\"internalrag-mini-datalineage\")\n",
    "\n",
    "# Initialize VectorStore\n",
    "vector_store_wiki = PineconeVectorStore(pinecone_index=wiki_index)\n",
    "vector_store_datalineage = PineconeVectorStore(pinecone_index=datalineage_index)\n",
    "\n",
    "# Instantiate VectorStoreIndex object from your vector_store object\n",
    "vector_index_wiki = VectorStoreIndex.from_vector_store(vector_store=vector_store_wiki)\n",
    "vector_index_datalineage = VectorStoreIndex.from_vector_store(vector_store=vector_store_datalineage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### React Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The question specifically asks for the source systems for a specific table, and the second choice is focused on answering questions about the source systems for specific tables..\n",
      "\u001b[0mThe source systems flowing into the dashboard of the Purchase Order include the tables \"dpor_purchase_order\" and \"fpor_dm_80_billed_purchase_order_all_v.\"\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question about the source systems for the tables \"dpor_purchase_order\" and \"fpor_dm_80_billed_purchase_order_all_v.\"\n",
      "Action: make_sql_query\n",
      "Action Input: {'table_names': ['dpor_purchase_order', 'fpor_dm_80_billed_purchase_order_all_v']}\n",
      "\u001b[0m\u001b[1;3;34mObservation: {'dpor_purchase_order':         LVL0_TARGETNAME DV_PHYSICAL_SCHEMA  \\\n",
      "0   dpor_purchase_order    emea_obis_dv_fl   \n",
      "1   dpor_purchase_order   emea_obis_ref_fl   \n",
      "2   dpor_purchase_order    emea_obis_dv_fl   \n",
      "3   dpor_purchase_order    emea_obis_dv_fl   \n",
      "4   dpor_purchase_order    emea_obis_dv_fl   \n",
      "5   dpor_purchase_order    emea_obis_dv_fl   \n",
      "6   dpor_purchase_order   emea_obis_ref_fl   \n",
      "7   dpor_purchase_order   emea_obis_ref_fl   \n",
      "8   dpor_purchase_order   emea_obis_ref_fl   \n",
      "9   dpor_purchase_order   emea_obis_ref_fl   \n",
      "10  dpor_purchase_order   emea_obis_ref_fl   \n",
      "11  dpor_purchase_order    emea_obis_dv_fl   \n",
      "12  dpor_purchase_order   emea_obis_ref_fl   \n",
      "13  dpor_purchase_order   emea_obis_ref_fl   \n",
      "14  dpor_purchase_order               NULL   \n",
      "15  dpor_purchase_order    emea_obis_dv_fl   \n",
      "16  dpor_purchase_order    emea_obis_dv_fl   \n",
      "17  dpor_purchase_order   emea_obis_ref_fl   \n",
      "18  dpor_purchase_order    emea_obis_dv_fl   \n",
      "19  dpor_purchase_order    emea_obis_dv_fl   \n",
      "20  dpor_purchase_order    emea_obis_dv_fl   \n",
      "21  dpor_purchase_order    emea_obis_dv_fl   \n",
      "22  dpor_purchase_order    emea_obis_dv_fl   \n",
      "\n",
      "                                        DV_TABLE_NAME SRC_PHYSICAL_SCHEMA  \\\n",
      "0             sat_emea_sapoe_mmpur_2lis02itm_purchase      RZ_SAPOE_MMPUR   \n",
      "1                           ref_emea_sap_oe_ref_tvstt        RZ_SAPOE_REF   \n",
      "2                sat_emea_sap_oe_sd_change_doc_header         RZ_SAPOE_SD   \n",
      "3             sat_emea_sapow_mmpur_2lis02itm_purchase      RZ_SAPOW_MMPUR   \n",
      "4   sat_emea_synapse_nav_osrdporpurchaseorder_purc...              RZ_NAV   \n",
      "5             sat_emea_sapoe_mmpur_2lis02hdr_purchase      RZ_SAPOE_MMPUR   \n",
      "6                           ref_emea_sap_ow_ref_tprit        RZ_SAPOW_REF   \n",
      "7                           ref_emea_sap_ow_ref_tbsgt        RZ_SAPOW_REF   \n",
      "8                           ref_emea_sap_ow_ref_dd07t        RZ_SAPOW_REF   \n",
      "9                           ref_emea_sap_oe_ref_dd07t        RZ_SAPOE_REF   \n",
      "10                          ref_emea_sap_ow_ref_tvstt        RZ_SAPOW_REF   \n",
      "11           sat_emea_sapoe_mmpur_vendor_confirmation      RZ_SAPOE_MMPUR   \n",
      "12                          ref_emea_sap_oe_ref_tbsgt        RZ_SAPOE_REF   \n",
      "13                          ref_emea_sap_crmref_dd07t       RZ_SAPCRM_REF   \n",
      "14                                               NULL                NULL   \n",
      "15                  sat_emea_sapoe_mmpur_transit_time      RZ_SAPOE_MMPUR   \n",
      "16            sat_emea_sapow_mmpur_2lis02scl_purchase      RZ_SAPOW_MMPUR   \n",
      "17                          ref_emea_sap_oe_ref_tprit        RZ_SAPOE_REF   \n",
      "18  sat_emea_synapse_nav_ocrdporpurchaseorder_purc...              RZ_NAV   \n",
      "19  sat_emea_synapse_nav_osldporpurchaseorder_purc...              RZ_NAV   \n",
      "20            sat_emea_sapoe_mmpur_2lis02scl_purchase      RZ_SAPOE_MMPUR   \n",
      "21            sat_emea_sapow_mmpur_2lis02hdr_purchase      RZ_SAPOW_MMPUR   \n",
      "22           sat_emea_sapoe_mmpur_change_doc_pos_ekko      RZ_SAPOE_MMPUR   \n",
      "\n",
      "             SRC_TABLE_NAME  \n",
      "0               2lis_02_itm  \n",
      "1                     tvstt  \n",
      "2                     cdhdr  \n",
      "3               2lis_02_itm  \n",
      "4   osr_dpor_purchase_order  \n",
      "5               2lis_02_hdr  \n",
      "6                     tprit  \n",
      "7                     tbsgt  \n",
      "8                     dd07t  \n",
      "9                     dd07t  \n",
      "10                    tvstt  \n",
      "11                     ekes  \n",
      "12                    tbsgt  \n",
      "13                    dd07t  \n",
      "14                     NULL  \n",
      "15          zmpr_trans_time  \n",
      "16              2lis_02_scl  \n",
      "17                    tprit  \n",
      "18  ocr_dpor_purchase_order  \n",
      "19  osl_dpor_purchase_order  \n",
      "20              2lis_02_scl  \n",
      "21              2lis_02_hdr  \n",
      "22               cdpos_ekko  , 'fpor_dm_80_billed_purchase_order_all_v':                            LVL0_TARGETNAME DV_PHYSICAL_SCHEMA  \\\n",
      "0   fpor_dm_80_billed_purchase_order_all_v    emea_obis_dv_fl   \n",
      "1   fpor_dm_80_billed_purchase_order_all_v    emea_obis_dv_fl   \n",
      "2   fpor_dm_80_billed_purchase_order_all_v    emea_obis_dv_fl   \n",
      "3   fpor_dm_80_billed_purchase_order_all_v               NULL   \n",
      "4   fpor_dm_80_billed_purchase_order_all_v    emea_obis_dv_fl   \n",
      "5   fpor_dm_80_billed_purchase_order_all_v    emea_obis_dv_fl   \n",
      "6   fpor_dm_80_billed_purchase_order_all_v    emea_obis_dv_fl   \n",
      "7   fpor_dm_80_billed_purchase_order_all_v    emea_obis_dv_fl   \n",
      "8   fpor_dm_80_billed_purchase_order_all_v    emea_obis_dv_fl   \n",
      "9   fpor_dm_80_billed_purchase_order_all_v    emea_obis_dv_fl   \n",
      "10  fpor_dm_80_billed_purchase_order_all_v    emea_obis_dv_fl   \n",
      "11  fpor_dm_80_billed_purchase_order_all_v    emea_obis_dv_fl   \n",
      "12  fpor_dm_80_billed_purchase_order_all_v   emea_obis_ref_fl   \n",
      "13  fpor_dm_80_billed_purchase_order_all_v   emea_obis_ref_fl   \n",
      "14  fpor_dm_80_billed_purchase_order_all_v    emea_obis_dv_fl   \n",
      "15  fpor_dm_80_billed_purchase_order_all_v    emea_obis_dv_fl   \n",
      "16  fpor_dm_80_billed_purchase_order_all_v    emea_obis_dv_fl   \n",
      "17  fpor_dm_80_billed_purchase_order_all_v    emea_obis_dv_fl   \n",
      "\n",
      "                                      DV_TABLE_NAME SRC_PHYSICAL_SCHEMA  \\\n",
      "0              lds_emea_synapse_hfm_prod_fact_rates              RZ_HFM   \n",
      "1   lds_emea_sap_oe_mm_material_plant_0matplantattr         RZ_SAPOE_MM   \n",
      "2       sat_emea_synapse_hfm_prodcustom1_hfm_custom              RZ_HFM   \n",
      "3                                              NULL                NULL   \n",
      "4           sat_emea_sap_ow_sd_2lis12vcitm_delivery         RZ_SAPOW_SD   \n",
      "5           sat_emea_sap_oe_sd_2lis12vcitm_delivery         RZ_SAPOE_SD   \n",
      "6      sat_emea_synapse_hfm_prodaccount_hfm_account              RZ_HFM   \n",
      "7     sat_emea_synapse_nav_ocrfpordelivery_delivery              RZ_NAV   \n",
      "8            sat_emea_synapse_hfm_prodview_hfm_view              RZ_HFM   \n",
      "9                sat_emea_sapoe_mmpur_ekbe_purchase      RZ_SAPOE_MMPUR   \n",
      "10       sat_emea_synapse_hfm_prodperiod_hfm_period              RZ_HFM   \n",
      "11   sat_emea_synapse_hfm_prodscenario_hfm_scenario              RZ_HFM   \n",
      "12  ref_emea_synapse_refdata_rinp_inventory_process          RZ_REFDATA   \n",
      "13           ref_emea_synapse_refdata_rcur_currency          RZ_REFDATA   \n",
      "14                lds_emea_sap_ow_mm_material_plant         RZ_SAPOW_MM   \n",
      "15    sat_emea_synapse_nav_osrfpordelivery_delivery              RZ_NAV   \n",
      "16    sat_emea_synapse_nav_oslfpordelivery_delivery              RZ_NAV   \n",
      "17         sat_emea_sapoe_mmpur_vendor_confirmation      RZ_SAPOE_MMPUR   \n",
      "\n",
      "             SRC_TABLE_NAME  \n",
      "0           prod_fact_rates  \n",
      "1           0mat_plant_attr  \n",
      "2              prod_custom1  \n",
      "3                      NULL  \n",
      "4             2lis_12_vcitm  \n",
      "5             2lis_12_vcitm  \n",
      "6              prod_account  \n",
      "7   ocr_fpor_purchase_order  \n",
      "8                 prod_view  \n",
      "9                      ekbe  \n",
      "10              prod_period  \n",
      "11            prod_scenario  \n",
      "12   rinp_inventory_process  \n",
      "13            rcur_currency  \n",
      "14          0mat_plant_attr  \n",
      "15  osr_fpor_purchase_order  \n",
      "16  osl_fpor_purchase_order  \n",
      "17                     ekes  }\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: The source systems flowing into the dashboard of the Purchase Order, as indicated by the source tables for \"dpor_purchase_order\" and \"fpor_dm_80_billed_purchase_order_all_v,\" include the following:\n",
      "\n",
      "For \"dpor_purchase_order\":\n",
      "- RZ_SAPOE_MMPUR with source tables such as 2lis_02_itm, 2lis_02_hdr, ekes, zmpr_trans_time, and cdpos_ekko.\n",
      "- RZ_SAPOE_REF with source tables like tvstt, tprit, tbsgt, and dd07t.\n",
      "- RZ_SAPOW_MMPUR with source tables such as 2lis_02_itm and 2lis_02_scl.\n",
      "- RZ_SAPOW_REF with source tables like tprit, tbsgt, tvstt, and dd07t.\n",
      "- RZ_SAPCRM_REF with the source table dd07t.\n",
      "- RZ_NAV with source tables osr_dpor_purchase_order, ocr_dpor_purchase_order, and osl_dpor_purchase_order.\n",
      "- RZ_SAPOE_SD with the source table cdhdr.\n",
      "\n",
      "For \"fpor_dm_80_billed_purchase_order_all_v\":\n",
      "- RZ_HFM with source tables prod_fact_rates, prod_custom1, prod_account, prod_view, prod_period, and prod_scenario.\n",
      "- RZ_SAPOE_MM with the source table 0mat_plant_attr.\n",
      "- RZ_SAPOW_SD with the source table 2lis_12_vcitm.\n",
      "- RZ_SAPOE_SD with the source table 2lis_12_vcitm.\n",
      "- RZ_NAV with source tables ocr_fpor_purchase_order, osr_fpor_purchase_order, and osl_fpor_purchase_order.\n",
      "- RZ_REFDATA with source tables rinp_inventory_process and rcur_currency.\n",
      "- RZ_SAPOE_MMPUR with the source table ekbe.\n",
      "- RZ_SAPOW_MM with the source table 0mat_plant_attr.\n",
      "\n",
      "These source systems represent the various schemas and tables from which data is sourced to populate the dashboard of the Purchase Order.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Current Version\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, load_index_from_storage, PromptTemplate\n",
    "from llama_index.core.tools import QueryEngineTool, BaseTool, FunctionTool, ToolMetadata\n",
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector, LLMMultiSelector\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.selectors import PydanticMultiSelector, PydanticSingleSelector\n",
    "import snowflake.connector as snowflake\n",
    "from snowflake.connector import DictCursor\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from utils import make_sql_query\n",
    "from llama_index.core.agent import AgentRunner\n",
    "from llama_index.agent.openai import OpenAIAgentWorker, OpenAIAgent\n",
    "\n",
    "# Loading Environment variables:\n",
    "dotenv_path = 'KEYs.env'  \n",
    "_ = load_dotenv(dotenv_path)\n",
    "\n",
    "# Query Engines\n",
    "vector_query_engine_wiki = vector_index_wiki.as_query_engine()\n",
    "vector_query_engine_datalineage = vector_index_datalineage.as_query_engine()\n",
    "\n",
    "# Create Query Engine Tool\n",
    "vector_tool_wiki = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine_wiki,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from wiki entries and naming conventions\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool_datalineage = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine_datalineage,\n",
    "    description=(\n",
    "        \"Useful only to answer questions regarding the source systems for specific tables (e.g. What are the source systems for table x, where does the information for table x come from, etc.).\" \n",
    "        \"the output of this query engine are NOT the final source systems, but only the correct Table Name(s) (i.e. LVL0_TargetName from the data catalogue). The correct LVL0_TargetName(s) by finding the most fitting table descriptions, based on the table name or description given in the user query.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Define Router Query Engine\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=PydanticSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        vector_tool_wiki,\n",
    "        vector_tool_datalineage,\n",
    "    ],\n",
    "    verbose=True\n",
    ")       \n",
    "\n",
    "# SET USER QUERY:\n",
    "#user_query = \"Explain the Micro Batch Loading Framework for the Curated Zone\"\n",
    "user_query = \"What are the source systems flowing into the dashbaord of the Purchase Order\"\n",
    "#user_query = \"What does RZ_SAPOE_MM and RZ_SAPOW_SD mean?\"\n",
    "\n",
    "response = query_engine.query(user_query)\n",
    "print(str(response))\n",
    "\n",
    "if response.metadata['selector_result'].selections[0].index == 1: # if Query Engine 1 is used (i.e. Data lineage) --> perform sql function call (always)\n",
    "    \n",
    "    # Create Function Tool\n",
    "    sql_query_tool = FunctionTool.from_defaults(fn=make_sql_query)\n",
    "\n",
    "    #llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0) # try gpt-3.5-turbo-0613 as well\n",
    "    llm = OpenAI(model=\"gpt-4-1106-preview\", temperature=0) # try gpt-3.5-turbo-0613 as well\n",
    "    #llm = OpenAI(model=\"gpt-4-1106-preview\", temperature=0) # Model name gpt-3.5-turbo-instruct does not support function calling API. \n",
    "\n",
    "    agent = ReActAgent.from_tools([sql_query_tool], llm=llm, verbose=True)\n",
    "    #agent = OpenAIAgent.from_tools([sql_query_tool], llm=llm, verbose=True)\n",
    "\n",
    "    final_response_template = f\"\"\"Given these preliminary table names identified in the \"Previous Reponse\", perform make_sql_query and answer the question given as \"Initial Question\": \n",
    "                                ---\n",
    "                                Initial Questions: {user_query}\n",
    "                                Previous Response: {response}\n",
    "                                ---\n",
    "                                SQL TABLE LEGEND: \n",
    "                                LvL0_TargetName: Name of the target table in the Data Mart and Curated Zone\n",
    "                                DV_Physical_Schema: Name of the Schema for the Vault and Standardized Zone\n",
    "                                DV_Table_Name: Name of the Table or Vault object in the Vault and Standardized Zone\n",
    "                                SRC_Physical_Schema: Name of the Schema for the Raw Zone table, containing the source systems\n",
    "                                SRC_Table_Name: Name of the Table in the Raw Zone, containing the table name in the respective source systems\n",
    "\n",
    "                                NOTE: If the Initial question is out of scope or has nothing to do with data lineage, output: \"Question is out of chatbot scope\".\n",
    "                                \"\"\"\n",
    "    response_final = agent.chat(final_response_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Router Query Engine Options\n",
    "**Option 1: Pyndantic Selector**\n",
    "\n",
    "The Pydantic selectors (currently only supported by gpt-4-0613 and gpt-3.5-turbo-0613 (the default)) use the OpenAI Function Call API to produce pydantic selection objects, rather than parsing raw JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector, LLMMultiSelector\n",
    "from llama_index.core.selectors import (\n",
    "    PydanticMultiSelector,\n",
    "    PydanticSingleSelector,\n",
    ")\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=PydanticSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        vector_tool_wiki,\n",
    "        vector_tool_datalineage,\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: The question requires specific context from wiki entries and naming conventions, which aligns with the purpose of this choice..\n",
      "\u001b[0mThe Micro Batch Loading Framework for the Curated Zone involves constantly adding new transactions to the Standardized Zone with a unique identifier and timestamp. The framework utilizes SQL queries to extract only the newer transactions based on the ascending timestamp. Replicated data may be slightly out-of-date compared to the Standardized Zone due to newly added transactions. Small batches are used to incrementally add transactions to the replicated table, resulting in a large amount of accumulated data over time. However, individual data loads only need to handle a small number of rows, making the incremental data extraction fast and enabling microbatching to closely approach real-time updates.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = query_engine.query(\"Explain the Micro Batch Loading Framework for the Curated Zone\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The question specifically asks for the source systems for a specific table, which aligns with the functionality described in choice (2)..\n",
      "\u001b[0mThe source systems flowing into the dashboard of the Purchase Order are the tables \"dpor_purchase_order\" and \"fpor_dm_80_billed_purchase_order_all_v.\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = query_engine.query(\"What are the source systems flowing into the dashbaord of the Purchase Order\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: LLMSingleSelector**\n",
    "\n",
    "The LLM selectors use the LLM to output a JSON that is parsed, and the corresponding indexes are queried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        vector_tool_wiki,\n",
    "        vector_tool_datalineage,\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: Useful for retrieving specific context from wiki entries and naming conventions.\n",
      "\u001b[0mThe Micro Batch Loading Framework for the Curated Zone involves constantly adding new transactions to the Standardized Zone with a unique identifier and timestamp. By leveraging the ascending nature of the transaction timestamps, it becomes easy to select only the newer transactions. This technique forms the basis of the SQL query used to extract new records from the Standardized Zone. The replicated data may be slightly out-of-date compared to the Standardized Zone due to the addition of new transactions, but small batches are continuously added to the replicated table, resulting in a small increment of transactions. This incremental approach allows for fast data extraction and enables microbatching to closely align with real-time updates.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Explain the Micro Batch Loading Framework for the Curated Zone\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: This tool should return the specific Table Name(s) by finding the most fitting table descriptions, based on the table name given in the user query..\n",
      "\u001b[0mThe source systems flowing into the dashboard of the Purchase Order are the tables \"dpor_purchase_order\" and \"fpor_dm_80_billed_purchase_order_all_v.\"\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What are the source systems flowing into the dashbaord of the Purchase Order\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selections=[SingleSelection(index=1, reason='This tool should return the specific Table Name(s) by finding the most fitting table descriptions, based on the table name given in the user query.')]\n"
     ]
    }
   ],
   "source": [
    "# [optional] look at selected results\n",
    "print(str(response.metadata[\"selector_result\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Node ID: e8887394-77bc-450a-ab33-84524b3c8f32\n",
      "Text: Dimensions  Last update: 2024.05.09 | Table Name | Associated\n",
      "Dimensions | Description | |:--|:--|:--| |dbid_billing_document|\n",
      "|Contains descriptive information about billing documents, payment\n",
      "status payment terms, and detailing transaction dates. |\n",
      "|dacc_account|  |Holds account information including account status,\n",
      "type and structure.| |dcus...\n",
      "Score:  0.395\n",
      "\n",
      "Node ID: 0a9cf475-06a9-4781-aef1-7371b76ccf3d\n",
      "Text: Fact | Table Name | Associated Dimensions | Description |\n",
      "|:--|:--|:--| |fcur_currency_rate_hyperion|dcur_currency|Stores\n",
      "currency exchange rates, and links the reference date, scenario, and\n",
      "rate type.| |ffco_cooo_cooc|  |Contains details of cost center\n",
      "operations, including cost allocations and related financials and\n",
      "links the relevant customer...\n",
      "Score:  0.373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(response.source_nodes))\n",
    "print(response.source_nodes[0])\n",
    "print(response.source_nodes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReAct Agents - Alternative System Architecture Idea (doesnt work yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LVL0_TARGETNAME</th>\n",
       "      <th>SRC_TABLE_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dbid_billing_document</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dbid_billing_document</td>\n",
       "      <td>vbfa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dbid_billing_document</td>\n",
       "      <td>zcrm_doc_flow_h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dbid_billing_document</td>\n",
       "      <td>t052u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dbid_billing_document</td>\n",
       "      <td>dd07t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dbid_billing_document</td>\n",
       "      <td>2lis_13_vditm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dbid_billing_document</td>\n",
       "      <td>0fi_gl_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dbid_billing_document</td>\n",
       "      <td>2lis_13_vditm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dbid_billing_document</td>\n",
       "      <td>2lis_13_vdhdr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dbid_billing_document</td>\n",
       "      <td>vbfa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dbid_billing_document</td>\n",
       "      <td>2lis_13_vdhdr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dbid_billing_document</td>\n",
       "      <td>dd07t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dbid_billing_document</td>\n",
       "      <td>dd07t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dbid_billing_document</td>\n",
       "      <td>0bea_crmb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dbid_billing_document</td>\n",
       "      <td>t052u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dbid_billing_document</td>\n",
       "      <td>0fi_gl_14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          LVL0_TARGETNAME   SRC_TABLE_NAME\n",
       "0   dbid_billing_document             NULL\n",
       "1   dbid_billing_document             vbfa\n",
       "2   dbid_billing_document  zcrm_doc_flow_h\n",
       "3   dbid_billing_document            t052u\n",
       "4   dbid_billing_document            dd07t\n",
       "5   dbid_billing_document    2lis_13_vditm\n",
       "6   dbid_billing_document        0fi_gl_14\n",
       "7   dbid_billing_document    2lis_13_vditm\n",
       "8   dbid_billing_document    2lis_13_vdhdr\n",
       "9   dbid_billing_document             vbfa\n",
       "10  dbid_billing_document    2lis_13_vdhdr\n",
       "11  dbid_billing_document            dd07t\n",
       "12  dbid_billing_document            dd07t\n",
       "13  dbid_billing_document        0bea_crmb\n",
       "14  dbid_billing_document            t052u\n",
       "15  dbid_billing_document        0fi_gl_14"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_table_names = ['dacc_account', 'dbid_billing_document']\n",
    "sql_result_dict = make_sql_query(list_of_table_names)\n",
    "#sql_result_dict['dacc_account']\n",
    "sql_result_dict['dbid_billing_document']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
